# Feature Specification: Testing Framework and Strategy

**Feature Branch**: `001-testing-framework`
**Created**: 2025-11-06
**Status**: Draft
**Input**: User description: "Testing Framework and Strategy - Establish comprehensive testing framework and strategy for all components of the PulumiCost system"
**GitHub Issue**: [#9](https://github.com/rshade/pulumicost-core/issues/9)

## User Scenarios & Testing *(mandatory)*

### User Story 1 - Unit Testing Foundation (Priority: P1)

As a PulumiCost developer, I need a comprehensive unit testing framework so that I can verify individual component logic in isolation and catch regressions early in development.

**Why this priority**: Unit tests are the foundation of test coverage. They provide fast feedback during development and are essential for achieving the 80% coverage requirement defined in the constitution. Without this foundation, developers cannot verify their code works correctly before integration.

**Independent Test**: Can be fully tested by running unit tests for any single package (e.g., `go test ./internal/engine/...`) and verifying coverage reports show 80%+ coverage with all tests passing.

**Acceptance Scenarios**:

1. **Given** a developer makes changes to the engine package, **When** they run unit tests, **Then** all tests pass and coverage is reported showing 80%+ coverage for the engine package
2. **Given** a new component is added to the CLI package, **When** unit tests are written following the framework, **Then** tests execute in under 1 second and verify component behavior in isolation
3. **Given** a developer runs the full unit test suite, **When** tests complete, **Then** coverage reports are generated showing per-package and overall coverage metrics

---

### User Story 2 - Integration Testing (Priority: P2)

As a PulumiCost developer, I need integration testing capabilities so that I can verify cross-component communication and plugin interactions work correctly before deploying changes.

**Why this priority**: After unit tests prove individual components work, integration tests verify they work together correctly. This is critical for plugin communication (gRPC), CLI command execution, and data flow between components. Integration tests catch issues that unit tests miss.

**Independent Test**: Can be fully tested by running integration test suite (`go test ./test/integration/...`) which spawns mock plugins, executes CLI commands, and verifies end-to-end behavior without requiring external dependencies.

**Acceptance Scenarios**:

1. **Given** a mock plugin is running, **When** the CLI executes a cost calculation command, **Then** gRPC communication succeeds and correct results are returned
2. **Given** multiple components are involved in a workflow, **When** integration tests execute, **Then** data flows correctly between components (ingest → engine → output) and all assertions pass
3. **Given** a configuration file is loaded, **When** integration tests run, **Then** the system correctly uses configuration values across all components

---

### User Story 3 - End-to-End Testing (Priority: P3)

As a PulumiCost quality engineer, I need end-to-end testing with real Pulumi plans so that I can verify complete workflows function correctly in production-like scenarios.

**Why this priority**: E2E tests provide the highest confidence that the system works correctly for users, but they are slower and more complex to maintain. They are lower priority than unit and integration tests because those provide faster feedback during development.

**Independent Test**: Can be fully tested by running E2E test suite (`go test ./test/e2e/...`) which uses real Pulumi plan JSON files, launches actual plugins, and verifies complete workflows produce correct output in all formats (table, JSON, NDJSON).

**Acceptance Scenarios**:

1. **Given** a real Pulumi plan JSON file for AWS resources, **When** the complete workflow executes (plan → ingest → calculate → output), **Then** costs are calculated correctly and output matches expected golden files
2. **Given** multiple output formats are requested, **When** E2E tests run, **Then** all formats (table, JSON, NDJSON) are generated correctly and validated
3. **Given** error scenarios occur (invalid JSON, plugin failure), **When** E2E tests execute, **Then** errors are handled gracefully with appropriate error messages

---

### User Story 4 - Mock Plugin Infrastructure (Priority: P2)

As a PulumiCost developer, I need configurable mock plugins so that I can test plugin communication and error scenarios without depending on external services.

**Why this priority**: Mock plugins are critical for both unit and integration testing. They enable testing plugin failure scenarios, performance testing, and development without actual cloud provider plugins. This enables the P2 integration tests and must be available early.

**Independent Test**: Can be fully tested by launching a mock plugin, configuring it with test responses, invoking gRPC methods, and verifying it returns configured data and handles error injection correctly.

**Acceptance Scenarios**:

1. **Given** a mock plugin is configured with cost data, **When** the engine queries it via gRPC, **Then** the mock returns configured responses and all gRPC communication succeeds
2. **Given** error injection is enabled on the mock plugin, **When** the engine queries it, **Then** the mock returns configured errors and the engine handles them appropriately
3. **Given** performance testing is enabled, **When** the mock plugin is queried repeatedly, **Then** it simulates realistic latency and throughput for benchmarking

---

### User Story 5 - Test Fixtures and Data (Priority: P3)

As a PulumiCost developer, I need comprehensive test fixtures (Pulumi plans, mock responses, configs) so that I can write consistent tests across the codebase without manually creating test data.

**Why this priority**: Test fixtures improve test maintainability and consistency, but tests can be written with inline data initially. Fixtures become more important as the test suite grows and duplication becomes a maintenance burden.

**Independent Test**: Can be fully tested by verifying fixture files exist in organized directories, can be loaded by tests, and cover all major cloud providers (AWS, Azure, GCP) and scenarios (simple, complex, error cases).

**Acceptance Scenarios**:

1. **Given** test fixtures are organized in `/test/fixtures/`, **When** a developer writes a new test, **Then** they can easily load appropriate fixtures without creating test data
2. **Given** fixtures exist for AWS, Azure, and GCP, **When** cross-provider tests run, **Then** all providers are tested with realistic data
3. **Given** error scenario fixtures exist, **When** error handling tests run, **Then** all edge cases are covered with representative malformed data

---

### User Story 6 - CI/CD Test Automation (Priority: P1)

As a PulumiCost maintainer, I need automated test execution in CI/CD so that all tests run on every commit and pull requests are blocked if tests fail or coverage drops below thresholds.

**Why this priority**: Automated testing in CI is critical for maintaining code quality and preventing regressions. This must be in place early (P1) to enforce the constitution's TDD requirements and prevent bad code from being merged.

**Independent Test**: Can be fully tested by pushing a commit, verifying CI runs all test categories (unit, integration, E2E), generates coverage reports, and blocks merge if coverage is below 80% or any tests fail.

**Acceptance Scenarios**:

1. **Given** a pull request is opened, **When** CI runs, **Then** all unit, integration, and E2E tests execute and results are reported in the PR
2. **Given** test coverage drops below 80%, **When** CI runs, **Then** the build fails and the PR is blocked from merging
3. **Given** any test fails, **When** CI runs, **Then** the failure is clearly reported with logs and the PR cannot be merged until fixed

---

### Edge Cases

- What happens when Pulumi plan JSON is malformed or uses unsupported schema versions?
- How does the system handle plugin crashes or hangs during testing?
- What happens when test fixtures are corrupted or missing?
- How are flaky tests detected and handled in CI?
- What happens when coverage reports cannot be generated due to tooling issues?
- How does the system handle concurrent test execution (race conditions)?

## Requirements *(mandatory)*

### Functional Requirements

- **FR-001**: System MUST provide a unit testing framework for all Go packages with coverage reporting
- **FR-002**: System MUST support integration testing for cross-component interactions including gRPC plugin communication
- **FR-003**: System MUST enable end-to-end testing with real Pulumi plan JSON files
- **FR-004**: System MUST provide configurable mock plugin implementations for testing
- **FR-005**: System MUST organize test fixtures in structured directories by category (plans, responses, configs, error cases)
- **FR-006**: System MUST generate coverage reports showing per-package and overall coverage percentages
- **FR-007**: System MUST support table-driven tests for testing multiple scenarios efficiently
- **FR-008**: System MUST enable golden file testing for output format validation (table, JSON, NDJSON)
- **FR-009**: System MUST provide benchmark tests for performance regression detection
- **FR-010**: System MUST run all tests automatically in CI/CD pipeline on every commit
- **FR-011**: System MUST block pull requests if test coverage drops below 80% overall
- **FR-012**: System MUST block pull requests if critical paths have less than 95% coverage
- **FR-013**: System MUST execute tests with race detection enabled to catch concurrency issues
- **FR-014**: System MUST provide clear test failure messages with context and logs
- **FR-015**: System MUST support parallel test execution to minimize CI runtime

### Key Entities

- **Test Suite**: Collection of tests organized by category (unit, integration, e2e) with metadata about coverage, execution time, and results
- **Mock Plugin**: Configurable gRPC service implementing the CostSource protocol with support for custom responses, error injection, and performance simulation
- **Test Fixture**: Reusable test data file (Pulumi plan, mock response, configuration) stored in organized directories with documentation
- **Coverage Report**: Detailed analysis of code coverage showing per-package, per-file, and overall coverage percentages with identification of uncovered code paths
- **Benchmark Result**: Performance measurement data from benchmark tests including execution time, memory allocation, and regression detection

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: Unit test suite achieves minimum 80% overall code coverage within 4 weeks of implementation
- **SC-002**: Critical paths (CLI entry points, engine calculation, plugin communication) achieve 95% test coverage within 4 weeks
- **SC-003**: All tests execute in under 5 minutes locally and under 10 minutes in CI
- **SC-004**: Integration tests successfully verify plugin communication with 100% success rate
- **SC-005**: End-to-end tests validate all three output formats (table, JSON, NDJSON) with golden file matching
- **SC-006**: Mock plugin supports at least 5 configurable response scenarios and 3 error injection types
- **SC-007**: Test fixtures cover all three major cloud providers (AWS, Azure, GCP) with at least 3 plans each
- **SC-008**: CI/CD pipeline executes all tests on every commit with results visible within 15 minutes
- **SC-009**: Pull requests are automatically blocked if coverage drops below 80% or any tests fail
- **SC-010**: Benchmark tests detect performance regressions greater than 20% compared to baseline
- **SC-011**: 100% of error handling code paths are covered by tests within 4 weeks
- **SC-012**: Test suite runs with race detection enabled and reports zero race conditions
